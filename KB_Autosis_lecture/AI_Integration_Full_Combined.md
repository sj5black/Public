# ğŸ¤– AI ë„ì… ë°©ì‹ ë¹„êµ ë° ì‹¤ì „ í™œìš© ê°•ì˜ìë£Œ (OpenAI vs Ollama)

## ğŸ¯ ê°•ì˜ ëª©í‘œ
AIë¥¼ í”„ë¡œì íŠ¸ì— ë„ì…í•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ë‘ ê°€ì§€ ëŒ€í‘œì ì¸ ì ‘ê·¼ë²•
<br>
- **ìƒìš© AI API ê¸°ë°˜(OpenAI ë“±)** ê³¼ **ì˜¤í”ˆì†ŒìŠ¤ ë¡œì»¬ LLM ê¸°ë°˜(Ollama ë“±)** ì˜ ì°¨ì´ë¥¼ ì´í•´í•˜ê³ , ê° ë°©ì‹ì˜ êµ¬ì¡°ì™€ ì½”ë“œ ì˜ˆì‹œë¥¼ í†µí•´ ì‹¤ë¬´ ì ìš©ë²•ì„ ìµíŒë‹¤.

---

## ğŸ©µ 1. ê¸°ì¡´ LLMëª¨ë¸ì˜ API í™œìš©

### ğŸ“Œ ê°œìš”
OpenAIì˜ ChatGPT APIë‚˜ Embedding APIë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ë‹¤. 
- í´ë¼ìš°ë“œ ê¸°ë°˜ì´ë¯€ë¡œ ëª¨ë¸ì„ ì§ì ‘ ì„¤ì¹˜í•  í•„ìš”ê°€ ì—†ë‹¤.
- ëŒ€ì‹  **API í‚¤ ë°œê¸‰**, **ê³¼ê¸ˆ êµ¬ì¡°**, **í”„ë¡¬í”„íŠ¸ ì„¤ê³„** ë“±ì„ ì´í•´í•´ì•¼ í•œë‹¤.

---

### ğŸ’¸ (1) ëª¨ë¸ë³„ í† í° ê³¼ê¸ˆ êµ¬ì¡°

#### OpenAI

| Model                 |  Input | Cached input |      Output | ìš©ë„               | ë¹„ê³                     |
| --------------------- | -----: | -----------: | ----------: | ---------------- | --------------------- |
| **gpt-5-pro**         | $15.00 |            â€“ | **$120.00** | ì´ˆëŒ€í˜• ì½”ë“œ/ì—ì´ì „íŠ¸, ì—°êµ¬ìš© | ìµœê³  ì„±ëŠ¥Â·ì´ˆê³ ê°€, ì—”í„°í”„ë¼ì´ì¦ˆ ì „ìš©ê¸‰ |
| **gpt-5**             |  $1.25 |       $0.125 |  **$10.00** | ê³ ë‚œë„ ì¶”ë¡ Â·ì½”ë”©Â·ì—ì´ì „íŠ¸   | ìµœì‹  í”Œë˜ê·¸ì‹­ í…ìŠ¤íŠ¸           |
| **gpt-5-chat-latest** |  $1.25 |       $0.125 |  **$10.00** | ì¼ë°˜ ëŒ€í™”Â·ì—…ë¬´ ìë™í™”     | gpt-5ì˜ ì±„íŒ… ìµœì í™” íŠ¸ë™      |
| **gpt-5-codex**       |  $1.25 |       $0.125 |  **$10.00** | ì½”ë“œ ìƒì„±Â·ë¦¬íŒ©í† ë§       | ì½”ë“œ íŠ¹í™” ëª¨ë¸              |
| **gpt-4o**            |  $2.50 |        $1.25 |  **$10.00** | ë©€í‹°ëª¨ë‹¬(í…ìŠ¤íŠ¸+ì´ë¯¸ì§€)    | ì‹¤ì‹œê°„Â·ë¹„ì „ ê°€ëŠ¥             |
| **gpt-4.1**           |  $2.00 |        $0.50 |   **$8.00** | ì•ˆì •ì  ê³ ì„±ëŠ¥ ì¼ë°˜ìš©      | 4.x ë¼ì¸ í”Œë˜ê·¸ì‹­           |
| **gpt-5-mini**        |  $0.25 |       $0.025 |   **$2.00** | ë¹„ìš© íš¨ìœ¨ RAG/ìš”ì•½/ìë™í™” | ë¹ ë¥´ê³  ì €ë ´, ëŒ€ëŸ‰ ì²˜ë¦¬ ì í•©      |
| **gpt-4.1-mini**      |  $0.40 |        $0.10 |   **$1.60** | ê²½ëŸ‰ ë¶„ì„Â·ìš”ì•½         | 4.1ì˜ ê²½ëŸ‰íŒ              |
| **gpt-4o-mini**       |  $0.15 |       $0.075 |   **$0.60** | ì €ë¹„ìš© ë©€í‹°ëª¨ë‹¬         | ì‹¤ì‹œê°„ UIÂ·ëª¨ë°”ì¼ ì¹œí™”         |
| **gpt-5-nano**        |  $0.05 |       $0.005 |   **$0.40** | ì´ˆì €ë¹„ìš© ë£°ê¸°ë°˜/ë³´ì¡° íƒœìŠ¤í¬  | ì§€ì—°Â·í’ˆì§ˆë³´ë‹¤ ë¹„ìš© ìµœìš°ì„         |
| **gpt-4.1-nano**      |  $0.10 |       $0.025 |   **$0.40** | ì´ˆì €ë¹„ìš© ê²½ëŸ‰ ì²˜ë¦¬       | ì†Œí˜• íƒœìŠ¤í¬Â·í”„ë¦¬ì»´í“¨íŠ¸ìš©         |

#### Gemini
| Model                          | Input (â‰¤0.2M) | Input (>0.2M) |           Output (â‰¤0.2M) | Output (>0.2M) | ìš©ë„                | ë¹„ê³                                                 |
| ------------------------------ | ------------: | ------------: | -----------------------: | -------------: | ----------------- | ------------------------------------------------- |
| **Gemini 2.5 Pro**             |         $1.25 |         $2.50 |                  **$10** |        **$15** | ê³ ì„±ëŠ¥ ë©€í‹°ëª¨ë‹¬ & ê³ ë‚œë„ ì¶”ë¡  | ì¼ê´„ API ì…ë ¥ $0.625 / ì¶œë ¥ $5 ì ìš©                       |
| **Gemini 2.5 Flash**           |         $0.30 |         $0.30 |                **$2.50** |      **$2.50** | ì¼ë°˜ ë©€í‹°ëª¨ë‹¬, ì†ë„/ë¹„ìš© ê· í˜• | ì¼ê´„ API ì…ë ¥ $0.15 / ì¶œë ¥ $1.25, í•™ìŠµ í† í° $5/1M           |
| **Gemini 2.5 Flash (Preview)** |         $0.15 |         $0.15 | $0.60 (ë¹„ì¶”ë¡ ) / $3.50 (ì¶”ë¡ ) |  $0.60 / $3.50 | ì‹¤í—˜ìš© Flash í”„ë¦¬ë·°     | 2025.07.15 ì¢…ë£Œ, ì¼ê´„ API ì§€ì› ($0.075 / $0.30 / $1.75) |
| **Gemini 2.5 Flash Live API**  |         $0.50 |         $0.50 |                **$2.00** |      **$2.00** | ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°Â·ëŒ€í™”       | ì˜¤ë””ì˜¤ ì¶œë ¥ $12, ì˜ìƒ/ì˜¤ë””ì˜¤ ì…ë ¥ $3                          |
| **Gemini 2.5 Flash Lite**      |         $0.10 |         $0.10 |                **$0.40** |      **$0.40** | ì´ˆì €ë¹„ìš© ë©€í‹°ëª¨ë‹¬         | ê°€ì¥ ì €ë ´í•œ Gemini ëª¨ë¸ (ì¼ê´„ API ì—†ìŒ)                      |

#### Grok
| Model                | ì…ë ¥ í† í° (1M) | ì¶œë ¥ í† í° (1M) | ë¹„ê³            |
| -------------------- | ---------: | ---------: | ------------ |
| **Grok-4**           |  **$3.00** | **$15.00** | í˜„ì¬ ì£¼ë ¥ ëª¨ë¸     |
| **Grok-4-fast**      |  **$0.20** |  **$0.50** | ì†ë„Â·ë¹„ìš© íš¨ìœ¨ì„± ì¤‘ì  |
| **Grok-3**           |  **$3.00** | **$15.00** | ì—”í„°í”„ë¼ì´ì¦ˆ ì‘ì—… ì í•© |
| **Grok-code-fast-1** |  **$0.20** |  **$1.50** | ì½”ë”© ì‘ì—… íŠ¹í™” ëª¨ë¸  |

í† í°(token): AI ì–¸ì–´ ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë‚˜ëˆ„ëŠ” ìµœì†Œ "ì˜ë¯¸" ë‹¨ìœ„   
1. ë¬¸ì¥ì„ ì‘ì€ ì¡°ê°(Token)ìœ¼ë¡œ ë¶„í•´í•˜ê³ 
2. ê° í† í°ì„ ê³ ìœ í•œ ìˆ«ì(ID)ë¡œ ë§¤í•‘í•˜ê³ 
3. ì´ IDë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•´ ì‹ ê²½ë§ì— ì§‘ì–´ë„£ëŠ”ë‹¤

í•œêµ­ì–´
```
í•œêµ­ì˜ AI ì‚°ì—… ë™í–¥ì„ ì•Œë ¤ì¤˜

"í•œ", "êµ­", "ì˜", "AI", "ì‚°", "ì—…", "ë™", "í–¥", "ì„", "ì•Œ", "ë ¤", "ì¤˜"
â†’ 12 tokens
```

ì˜ì–´
```
Summarize the AI industry trends in Korea

["Summ", "arize"], "the", "AI", "industry", "trends", "in", "Korea"
â†’ 8 tokens
```
â†’ í†µìƒì ìœ¼ë¡œ ê°™ì€ í‘œí˜„ì„ ì¶œë ¥í•  ë•Œ ì˜ì–´ê°€ í•œêµ­ì–´ë³´ë‹¤ ë” ì ì€ í† í° ì†Œëª¨

#### OpenAI í™œìš© ì˜ˆì‹œ

```python
# OpenAI
import os
from dotenv import load_dotenv
from openai import OpenAI

# 1. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY ê°€ .env ì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

# 2. í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = OpenAI(api_key=api_key)

def ask_ai(query: str) -> str:
    """
    ì‚¬ìš©ìì˜ ì§ˆë¬¸(query)ì„ ë°›ì•„ OpenAI gpt-5-nanoë¡œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        response = client.chat.completions.create(
            model="gpt-5-nano",
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful assistant specialized in Korean IT and AI industry trends."
                },
                {
                    "role": "user",
                    "content": query
                }
            ]
        )
        # ì²« ë²ˆì§¸ ì‘ë‹µ ë©”ì‹œì§€ì˜ content ì¶”ì¶œ
        return response.choices[0].message.content
    except Exception as e:
        print(f"[ERROR] OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì£„ì†¡í•˜ì§€ë§Œ ì§€ê¸ˆì€ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

if __name__ == "__main__":
    # 3. ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸ ì…ë ¥ ë°›ê¸°
    user_query = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: í•œêµ­ì˜ AI ì‚°ì—… ë™í–¥ ìš”ì•½): ")

    # 4. AIì—ê²Œ ì§ˆë¬¸ ì „ë‹¬ í›„ ì‘ë‹µ ë°›ê¸°
    answer = ask_ai(user_query)

    # 5. ì‘ë‹µ ì¶œë ¥
    print("\n[AI ì‘ë‹µ]")
    print(answer)
```

---

### ğŸ§  (2) í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§

#### ğŸ§© ê°œë…
LLMì€ ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ ëª…ë ¹ì— ê°€ì¥ ì˜ ë°˜ì‘í•œë‹¤. í”„ë¡¬í”„íŠ¸ëŠ” ëª¨ë¸ì˜ **ì§€ì‹œë¬¸(Instruction)** ì—­í• ì„ í•œë‹¤.

#### ğŸ”¹ ê¸°ë³¸ ì˜ˆì‹œ
```python
from openai import OpenAI
client = OpenAI()

response = client.chat.completions.create(
  model="gpt-5-nano",
  messages=[
    {"role": "system", "content": "You are a helpful assistant for research."},
    {"role": "user", "content": "Summarize Teslaâ€™s Q3 earnings highlights."}
  ]
)
print(response.choices[0].message.content)
```

#### ğŸ”¹ ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° (4ë‹¨ê³„ êµ¬ì¡°)
```
[ROLE] â†’ ëª¨ë¸ ì—­í•  ì •ì˜
[CONTEXT] â†’ ì°¸ê³ í•  ë°°ê²½ ì •ë³´
[TASK] â†’ ìˆ˜í–‰í•  êµ¬ì²´ì  ëª©í‘œ
[OUTPUT FORMAT] â†’ ì¶œë ¥ í˜•ì‹ ëª…ì‹œ

ì˜ˆì‹œ:
ROLE: You are a financial analyst.
CONTEXT: Teslaâ€™s Q3 2024 report
TASK: Summarize key metrics (Revenue, EPS, Guidance)
OUTPUT FORMAT: Markdown table
```

#### Oneâ€‘shot
```python
demo = (
  "ì…ë ¥: ë¦¬ë·° â€” 'ë°°í„°ë¦¬ê°€ ì˜¤ë˜ê°€ê³  í™”ë©´ì´ ì„ ëª…í•˜ë‹¤'\n"
  "ì¶œë ¥: ê¸ì • | ì´ìœ : ë°°í„°ë¦¬ ìˆ˜ëª…, í™”ë©´ í’ˆì§ˆ"
)
messages=[
  {"role":"system","content":"You are a sentiment tagger. 'ê¸ì •/ë¶€ì •'ê³¼ 'ì´ìœ 'ë§Œ ì¶œë ¥."},
  {"role":"user","content": demo},
  {"role":"user","content":"ì…ë ¥: ë¦¬ë·° â€” 'ë°œì—´ì´ ì‹¬í•˜ê³  ê°€ê²©ì´ ë¹„ì‹¸ë‹¤'"}
]
print(client.chat.completions.create(model="gpt-5-mini", messages=messages)
      .choices[0].message.content)
```

#### Fewâ€‘shot
```python
shots=[("í’ˆì§ˆì´ ë”ì°í•˜ê³  í™˜ë¶ˆí–ˆë‹¤","ë¶€ì •"),
       ("ë°°ì†¡ì´ ë¹¨ëê³  í¬ì¥ì´ ì™„ë²½í–ˆë‹¤","ê¸ì •")]
msgs=[{"role":"system","content":"ë¬¸ì¥ì„ ê¸ì •/ì¤‘ë¦½/ë¶€ì •ìœ¼ë¡œ ë¶„ë¥˜"}]
for x,y in shots:
    msgs+=[{"role":"user","content":x},{"role":"assistant","content":y}]
msgs.append({"role":"user","content":"ê°€ê²©ì€ ë†’ì§€ë§Œ ë§Œì¡±ìŠ¤ëŸ½ë‹¤"})
print(client.chat.completions.create(model="gpt-5-mini", messages=msgs)
      .choices[0].message.content)
```

#### JSON/ìŠ¤í‚¤ë§ˆ ì œì•½
```python
schema = {
  "type":"object",
  "properties":{
    "label":{"type":"string","enum":["pos","neg","neu"]},
    "evidence":{"type":"array","items":{"type":"string"}}
  },
  "required":["label"]
}
client.chat.completions.create(
  model="gpt-5-mini",
  messages=[{"role":"user","content":"ë¬¸ì¥: 'ë°°ì†¡ ë¹ ë¥´ê³  í¬ì¥ ë¶ˆëŸ‰' ê°ì„± ë¼ë²¨ë§"}],
  response_format={"type":"json_schema","json_schema":schema}
)
```
---

### âš™ï¸ (3) ì£¼ìš” íŒŒë¼ë¯¸í„° ì„¤ëª…

| íŒŒë¼ë¯¸í„° | ì„¤ëª… | ê¶Œì¥ ë²”ìœ„ | íŠ¹ì§• |
|-----------|------|-----------|------|
| `temperature` | ì°½ì˜ì„± ì œì–´ | 0.2~0.8 | ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„±â†‘ |
| `top_p` | ëˆ„ì  í™•ë¥  ì»· | 0.7~0.95 | ë‹¤ì–‘ì„± ì œì–´ |
| `top_k` | ìƒìœ„ í›„ë³´ ê°œìˆ˜ ì»· | 10~50 | ëœë¤ì„± ì œì–´ |
| `max_tokens` | ì¶œë ¥ ìµœëŒ€ ê¸¸ì´ | ëª¨ë¸ë³„ ìƒí•œ | ë¬¸ì¥ ê¸¸ì´ ì œí•œ |
| `presence_penalty` | ìƒˆë¡œìš´ ì£¼ì œ ìœ ë„ | 0~2 | ë°˜ë³µ ë°©ì§€ |
| `frequency_penalty` | ë‹¨ì–´ ë°˜ë³µ ì–µì œ | 0~2 | ë‹¤ì–‘ì„± í™•ë³´ |

> âœ… RAG ì±—ë´‡ì—ëŠ” ì¼ë°˜ì ìœ¼ë¡œ `temperature=0.3`, `top_p=0.85`, `top_k=20`ì´ ì•ˆì •ì ì´ë‹¤.

---

## ğŸ§¡ 2. ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ LLM (Ollama)

### ğŸ“Œ ê°œìš”
- ëª¨ë¸ì„ **ë¡œì»¬ í™˜ê²½(GPU/CPU)** ì— ì§ì ‘ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì‹¤í–‰.
- ì™„ì „í•œ ì˜¤í”„ë¼ì¸ í™˜ê²½ì—ì„œë„ ì‘ë™ ê°€ëŠ¥.
- **ë¹„ìš© ì—†ìŒ**, í•˜ì§€ë§Œ GPU ì„±ëŠ¥ê³¼ ë©”ëª¨ë¦¬ ê´€ë¦¬ í•„ìš”.

---

### âš–ï¸ (1) ì£¼ìš” ëª¨ë¸ ë¹„êµ

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° ìˆ˜ | íŠ¹ì§• | ê¶Œì¥ ìš©ë„ |
|------|--------------|------|------------|
| **llama3:8b / 70b** | 8B~70B | Meta ê°œë°œ, ì˜ì–´ ì„±ëŠ¥ ìš°ìˆ˜ | ì¼ë°˜ QA, ì½”ë”© |
| **mistral:7b** | 7B | ì†ë„ ë¹ ë¥´ê³  íš¨ìœ¨ì  | ëŒ€í™”í˜• ì±—ë´‡ |
| **phi3:3.8b** | 3.8B | ê²½ëŸ‰, CPU êµ¬ë™ ê°€ëŠ¥ | ì €ì‚¬ì–‘ ì¥ë¹„ |
| **exaone3.5** | â€“ | í•œêµ­ì–´ ìµœì í™” | í•œê¸€ RAG, ë¡œì»¬ ì„œë¹„ìŠ¤ |
| **gemma2:9b** | 9B | Google ëª¨ë¸, ì¼ê´€ì„± ìš°ìˆ˜ | ë¬¸ì„œ ìš”ì•½ |

#### ğŸ’¡ íŒŒë¼ë¯¸í„°(Parameters)ë€?
- LLMì—ì„œì˜ ê° ë‰´ëŸ° ê°„ ì—°ê²°ë˜ëŠ” ê°€ì¤‘ì¹˜(weight) ì •ë„
- floatê°’ìœ¼ë¡œ í‘œí˜„ë˜ë©°, ë¬¸ì¥ì„ í•´ì„í•  ë•Œ í•´ë‹¹ ìˆ˜ì¹˜ë“¤ì„ ì¡°í•©í•´ "ë‹¤ìŒ ë‹¨ì–´(í† í°)ì˜ í™•ë¥ "ì„ ê³„ì‚°
- íŒŒë¼ë¯¸í„°ê°€ ë§ì„ìˆ˜ë¡ ë” ë§ê³  ì„¸ë°€í•œ ì–¸ì–´ì  íŒ¨í„´ êµ¬ì‚¬ê°€ ê°€ëŠ¥í•˜ì§€ë§Œ, vRAM ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ë„ ë†’ì•„ì§

### ğŸ†• (2) GPU/CUDA í™œìš©

ì˜¤í”ˆì†ŒìŠ¤ LLMì„ ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œ êµ¬ë™í•˜ë©´ ê°€ì¥ ë¨¼ì € ë¶€ë”ªíˆëŠ” ë¬¸ì œëŠ” **ì†ë„ì™€ ìì›(íŠ¹íˆ VRAM)** ì´ë‹¤.  
ëŒ€ë¶€ë¶„ì˜ LLM ì—°ì‚°ì€ **Self-Attention**ê³¼ **ëŒ€ê·œëª¨ í–‰ë ¬ ê³±(MatMul)** ë¡œ ì´ë£¨ì–´ì ¸ ìˆìœ¼ë©°, ì´ëŠ” ë™ì¼í•œ ì—°ì‚°ì„ ìˆ˜ì²œ ë²ˆ ë°˜ë³µ ìˆ˜í–‰í•˜ëŠ” í˜•íƒœë‹¤.  
ì´ë•Œ GPUì˜ **ë³‘ë ¬ ì—°ì‚° êµ¬ì¡°(CUDA)** ê°€ ë¹›ì„ ë°œí•œë‹¤. GPUëŠ” ìˆ˜ì²œ~ìˆ˜ë§Œ ê°œì˜ ì½”ì–´ë¡œ ë™ì‹œì— ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê³ , ê³ ì† GDDR/HBM ë©”ëª¨ë¦¬ë¥¼ í†µí•´ CPU ëŒ€ë¹„ í›¨ì”¬ ë„“ì€ ëŒ€ì—­í­ì„ ì œê³µí•œë‹¤.
ë”°ë¼ì„œ LLM ì¶”ë¡  ì‹œ GPUë¥¼ í™œìš©í•˜ë©´ **ì§€ì—°(latency)ì´ ê¸‰ê°**í•˜ê³ , ë™ì‹œì— **ì²˜ë¦¬ëŸ‰(throughput)** ë„ ê·¹ì ìœ¼ë¡œ ì¦ê°€í•œë‹¤.

ë˜í•œ GPUëŠ” **í˜¼í•©ì •ë°€ë„(FP16/BF16)**, **í…ì„œ ì½”ì–´(Tensor Core)**, **ì–‘ìí™”(Quantization, gguf 4bit/6bit)** ë“±ì„ í†µí•´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ë©´ì„œë„ ì—°ì‚° íš¨ìœ¨ì„ ë†’ì¸ë‹¤. OllamaëŠ” ì´ëŸ¬í•œ ìµœì í™”ë¥¼ ìë™ìœ¼ë¡œ ì ìš©í•´ GPUê°€ ì¡´ì¬í•˜ë©´ CUDAë¡œ ì „í™˜í•˜ê³ , ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ CPU fallbackì„ ìˆ˜í–‰í•œë‹¤.  
ê²°ê³¼ì ìœ¼ë¡œ, **ê°™ì€ í•˜ë“œì›¨ì–´ì—ì„œë„ ì†ë„ë¥¼ ë†’ì´ê³  VRAM íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°**ê°€ ëœë‹¤.

**í˜¼í•©ì •ë°€ë„**: ê¸°ì¡´ FP32ë¡œë§Œ êµ¬í˜„ëœ ì €ì¥ë°©ì‹ì„ ì •ë³´ì¤‘ìš”ë„ì— ë”°ë¼ FP32ë‘ FP16/BF16ì„ ì„ì–´ì„œ   
â†’ VRAM ëœ ì“°ê³ , ìˆ˜ì¹˜ ì•ˆì •ì„± ìœ ì§€í•˜ë©´ì„œ ë¹ ë¥´ê²Œ ê³„ì‚°í•˜ëŠ” í…Œí¬ë‹‰  
â†’ ëª¨ë¸ íŒŒë¼ë¯¸í„° / activation: FP16 ë˜ëŠ” BF16   
â†’ Gradient accumulation, ì¼ë¶€ optimizer ë‚´ë¶€ ê³„ì‚°, loss: FP32 ìœ ì§€   
â†’ FP = Floating Point (ex: 3.141592, 2367.235, 106903.1)   
â†’ BF = Brain Floating Point (bfloat16)

**í…ì„œ ì½”ì–´**: FP16/BF16 í–‰ë ¬ ê³± ì—°ì‚°ì— íŠ¹í™”ëœ ë”¥ëŸ¬ë‹ ì „ìš© í•˜ë“œì›¨ì–´ ìœ ë‹›   
â†’ ì¼ë°˜ CUDA ì½”ì–´ë¡œ FP32 í–‰ë ¬ì—°ì‚° ì‹œ í´ë¡ ë‹¹ ì²˜ë¦¬í•˜ëŠ” ì—°ì‚°ê°œìˆ˜ê°€ ì œí•œì ì¸ë°, í…ì„œì½”ì–´ëŠ” ì‘ì€ í–‰ë ¬ë¸”ë¡ì„ í•œë²ˆì— ì²˜ë¦¬í•˜ì—¬ í›¨ì”¬ ë¹ ë¦„

**ì–‘ìí™”**: í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ê·¹ë‹¨ì ìœ¼ë¡œ ì••ì¶•í•´ì„œ ëª¨ë¸ í¬ê¸°Â·VRAMì„ í™• ì¤„ì´ë©´ì„œë„, ì„±ëŠ¥ì€ ì–´ëŠ ì •ë„ ìœ ì§€í•˜ëŠ” í…Œí¬ë‹‰   
â†’ ê¸°ì¡´ 32ë¹„íŠ¸ ì²´ê³„ì˜ ì‹¤ìˆ˜ì •ë³´ë¥¼ 8bit, 6bit, 4bit ë‹¨ìœ„ë¡œ ì¶•ì†Œ (ë©”ëª¨ë¦¬ ì ˆê°)   
â†’ ì¤‘ìš”ë„ê°€ ë†’ì€ weightë§Œ ì •ë°€ë„ë¥¼ ìœ ì§€í•˜ê³ , ê·¸ ì™¸ì˜ ì •ë³´ë§Œ ì••ì¶•í•˜ê¸° ë–„ë¬¸ì— ë©”ëª¨ë¦¬ íš¨ìœ¨ ëŒ€ë¹„ ì„±ëŠ¥ì´ í¬ê²Œ ë³€í•˜ì§€ ì•ŠìŒ

---

### âš™ï¸ (3) ì„ë² ë”© â†” ìƒì„±(LLM) ë¶„ë¦¬

RAG íŒŒì´í”„ë¼ì¸ì—ì„œ **ì„ë² ë”©(Encoder)** ê³¼ **ìƒì„±(Decoder)** ì€ ì„±ê²©ì´ ì™„ì „íˆ ë‹¤ë¥´ë‹¤.

- **ì„ë² ë”©(Embedding)** ì€ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë°”ê¾¸ëŠ” **ë‹¨ë°œì„± ì—°ì‚°**ì´ ë§ì•„ CPUì— ì í•©í•˜ë‹¤.  
- **ìƒì„±(Generation)** ì€ í† í°ì„ ë°˜ë³µì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ê³¼ì •ì´ë¯€ë¡œ, GPU ë³‘ë ¬í™” íš¨ê³¼ê°€ í¬ë‹¤.

ë”°ë¼ì„œ ë³´í†µì€ **ì„ë² ë”©=CPU / ìƒì„±=GPU** ë¡œ ë¶„ë¦¬í•œë‹¤.  
ì´ë ‡ê²Œ í•˜ë©´ GPU ë¦¬ì†ŒìŠ¤ë¥¼ ìƒì„±ì— ì§‘ì¤‘ì‹œì¼œ **VRAMì„ ì ˆì•½**í•˜ê³ , CPUëŠ” ë°°ì¹˜ ì„ë² ë”©ì´ë‚˜ ìƒ‰ì¸ ê²€ìƒ‰ ë“± I/O ì¤‘ì‹¬ ì‘ì—…ì„ ì²˜ë¦¬í•œë‹¤.  
ë˜í•œ ì„ë² ë”©ì„ í´ë¼ìš°ë“œ API(OpenAI Embeddings ë“±)ë¡œ ë„˜ê¸°ê³ , ìƒì„±ë§Œ ë¡œì»¬ GPUì—ì„œ ëŒë¦¬ëŠ” **í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°**ë„ í”í•˜ë‹¤.


#### ğŸ’¡ ì˜ˆì‹œ: CPU ì„ë² ë”© + GPU LLM ì¶”ë¡ 

```python
from sentence_transformers import SentenceTransformer
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# 1ï¸âƒ£ CPUë¡œ ì„ë² ë”© (ë¬¸ì„œ ë²¡í„°í™”)
embed_model = SentenceTransformer("intfloat/multilingual-e5-base", device="cpu")
vec = embed_model.encode(["ìŠ¤ë§ˆíŠ¸ ë¬¼ë¥˜ í˜ì‹  ì „ëµ"], normalize_embeddings=True)

# 2ï¸âƒ£ GPUë¡œ LLM ì¶”ë¡  (FP16ìœ¼ë¡œ VRAM ì ˆì•½)
device = "cuda" if torch.cuda.is_available() else "cpu"
tok = AutoTokenizer.from_pretrained("meta-llama/Llama-3-8B-Instruct")
llm = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8B-Instruct",
    torch_dtype=torch.float16,
    device_map={"": device}
)
inputs = tok("ìš”ì•½: ìŠ¤ë§ˆíŠ¸ ë¬¼ë¥˜ í˜ì‹  ì „ëµì˜ í•µì‹¬ 3ì¤„.", return_tensors="pt").to(device)
print(tok.decode(llm.generate(**inputs, max_new_tokens=160)[0], skip_special_tokens=True))
```


### âš™ï¸ (4) ë‹¤ì¤‘ ì¸ìŠ¤í„´ìŠ¤ - ë¡œë“œ ë°¸ëŸ°ì‹±

####  Ollamaì—ì„œ ë‹¤ì¤‘ ì¸ìŠ¤í„´ìŠ¤(Multi-Instance)ê°€ í•„ìš”í•œ ì´ìœ 

####  1. LLMì€ â€œì‹±ê¸€ ìŠ¤ë ˆë“œ ì¶”ë¡ (single-thread inference)â€ êµ¬ì¡°
ëŒ€ë¶€ë¶„ì˜ LLMì€ ë‚´ë¶€ì ìœ¼ë¡œ **í† í°ì„ í•œ ë²ˆì— í•˜ë‚˜ì”© ìƒì„±**í•˜ëŠ” êµ¬ì¡°ë‹¤.  
ì¦‰, í•œ ìš”ì²­ì´ ë“¤ì–´ì˜¤ë©´ ëª¨ë¸ì´ **ìˆœì°¨ì ìœ¼ë¡œ í† í°ì„ ìƒì„±í•´ì•¼ í•˜ë©°**,  
ì´ ê³¼ì • ë™ì•ˆì—ëŠ” í•´ë‹¹ GPU ìì›ì´ **ê·¸ ìš”ì²­ì—ë§Œ ì ìœ (lock)** ëœë‹¤.

> ğŸ”¸ ì˜ˆì‹œ  
> - `generate()` í•¨ìˆ˜ëŠ” í•œ ë¬¸ì¥ì„ ë‹¤ ìƒì„±í•  ë•Œê¹Œì§€ GPUë¥¼ ë…ì í•¨  
> - ë‹¤ë¥¸ ì‚¬ìš©ìê°€ ë™ì‹œì— ìš”ì²­ì„ ë³´ë‚´ë©´ **ëŒ€ê¸°(queue)** ìƒíƒœë¡œ ë°€ë ¤ë‚¨  

ë”°ë¼ì„œ ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤(Ollama ì„œë²„ 1ê°œ)ë¡œëŠ” **ë™ì‹œ ìš”ì²­ ì²˜ë¦¬(Concurrency)** ê°€ ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜  
**ì‹¬ê°í•œ ì§€ì—°(latency spike)** ì´ ë°œìƒí•˜ê²Œ ëœë‹¤.

---

#### 2. ë‹¤ì¤‘ ì¸ìŠ¤í„´ìŠ¤ë¡œ â€œë³‘ë ¬ ì²˜ë¦¬(Pipelining)â€ ê°€ëŠ¥
ì—¬ëŸ¬ Ollama ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë„ìš°ë©´(ì˜ˆ: í¬íŠ¸ 11434, 11435, 11436),  
ê° ì¸ìŠ¤í„´ìŠ¤ê°€ **ì„œë¡œ ë…ë¦½ì ì¸ LLM í”„ë¡œì„¸ìŠ¤**ë¡œ ì‘ë™í•œë‹¤.  
ì¦‰, í•œ ì¸ìŠ¤í„´ìŠ¤ê°€ A ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ë™ì•ˆ  
ë‹¤ë¥¸ ì¸ìŠ¤í„´ìŠ¤ê°€ B, C, D ìš”ì²­ì„ ë™ì‹œì— ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.

ì´ êµ¬ì¡°ëŠ” **íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬(pipelined parallelism)** ì— ê°€ê¹ê³ ,  
ì‹¤ì§ˆì ìœ¼ë¡œ **API ì„œë²„ì˜ ë™ì‹œì²˜ë¦¬ëŸ‰(QPS)** ì„ 3ë°° ì´ìƒ í–¥ìƒì‹œí‚¨ë‹¤.

> ğŸ’¡ ì¦‰, â€œí•œ ëª¨ë¸ ì—¬ëŸ¬ ëª…ì´ ë™ì‹œì— ì“´ë‹¤â€ëŠ” êµ¬ì¡°ê°€ ì•„ë‹ˆë¼  
> â€œëª¨ë¸ í”„ë¡œì„¸ìŠ¤ë¥¼ ì—¬ëŸ¬ ê°œ ë„ì›Œì„œ ë¶„ë‹´ì‹œí‚¨ë‹¤â€ëŠ” ê°œë…ì´ë‹¤.

---

#### 3. GPU í™œìš© íš¨ìœ¨ ê·¹ëŒ€í™”
LLMì€ GPU ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì°¨ì§€í•˜ì§€ë§Œ í•­ìƒ ëª¨ë“  GPU ì½”ì–´ë¥¼ 100% ì“°ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤.  
ëª¨ë¸ í¬ê¸°ë‚˜ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ì— ë”°ë¼ **GPU ìœ íœ´ ì‹œê°„(idle time)** ì´ ë°œìƒí•œë‹¤.  
ì´ë•Œ ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë³‘ë ¬ë¡œ ìš´ì˜í•˜ë©´ GPU ìì›ì„ ë³´ë‹¤ **ê· ë“±í•˜ê²Œ í™œìš©(load balancing)** í•  ìˆ˜ ìˆë‹¤.

ì˜ˆ:
- 1ê°œì˜ Llama-3-8B ì¸ìŠ¤í„´ìŠ¤ â†’ GPU ì‚¬ìš©ë¥  45~60%  
- 3ê°œì˜ Llama-3-8B ì¸ìŠ¤í„´ìŠ¤ â†’ í‰ê·  ì‚¬ìš©ë¥  85~95%  
â†’ ê°™ì€ GPUì—ì„œ ë” ë§ì€ ìš”ì²­ì„ ë¹ ë¥´ê²Œ ì²˜ë¦¬ ê°€ëŠ¥

---

#### 4. ë¶€í•˜ ë¶„ì‚°(Load Balancing) ë° ì¥ì•  ëŒ€ì‘
ë‹¤ì¤‘ ì¸ìŠ¤í„´ìŠ¤ í™˜ê²½ì—ì„œëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ ë ˆë²¨ì—ì„œ  
`round-robin`, `asyncio.gather`, `FastAPI BackgroundTask` ë“±ì„ ì´ìš©í•´  
ê° ìš”ì²­ì„ **ëœë¤ ë˜ëŠ” ìˆœì°¨ ë¶„ë°°**í•  ìˆ˜ ìˆë‹¤.

#### 1. í•œê°œì˜ ì„œë²„ PCì—ì„œ ì—¬ëŸ¬ê°œì˜ ollama ì¸ìŠ¤í„´ìŠ¤(ì˜ˆ : 3ê°œì˜ í¬íŠ¸)ë¥¼ ë¶„ë¦¬í•˜ì—¬ ë¶„ì‚°ì²˜ë¦¬.

```python
# =========================================
# âœ… Ollama ì—¬ëŸ¬ í¬íŠ¸ë¡œ ë¶„ì‚° ìš”ì²­ ì˜ˆì‹œ
# =========================================
import requests
import itertools
import concurrent.futures

# í•œ PC ë‚´ ì„œë¡œ ë‹¤ë¥¸ í¬íŠ¸ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ Ollama ì¸ìŠ¤í„´ìŠ¤
OLLAMA_INSTANCES = [
    "http://127.0.0.1:11434",
    "http://127.0.0.1:11435",
    "http://127.0.0.1:11436"
]

# ë¼ìš´ë“œë¡œë¹ˆ ì œë„ˆë ˆì´í„°
round_robin = itertools.cycle(OLLAMA_INSTANCES)

def ask_local_ollama(prompt: str, model="exaone3.5"):
    server = next(round_robin)
    payload = {
        "model": model,
        "prompt": prompt,
        "options": {"temperature": 0.3, "top_p": 0.9}
    }
    res = requests.post(f"{server}/api/generate", json=payload, timeout=120)
    res.raise_for_status()
    answer = res.json().get("response")
    print(f"[{server}] ì‘ë‹µ ì™„ë£Œ")
    return server, answer

# í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
prompts = [
    "í•œêµ­ì˜ AI ì‚°ì—… í˜„í™©ì„ ìš”ì•½í•´ì¤˜.",
    "ì¸ê³µì§€ëŠ¥ ìŠ¤íƒ€íŠ¸ì—…ì˜ ì£¼ìš” íˆ¬ì íŠ¸ë Œë“œëŠ”?",
    "ì •ë¶€ì˜ AI ì •ì±… ë°©í–¥ì„ ê°„ë‹¨íˆ ì„¤ëª…í•´ì¤˜."
]

# ë™ì‹œì— 3ê°œ ìš”ì²­ ë³´ë‚´ê¸° (ThreadPoolExecutor í™œìš©)
with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
    futures = [executor.submit(ask_local_ollama, p) for p in prompts]
    for future in concurrent.futures.as_completed(futures):
        server, result = future.result()
        print(f"\nğŸ–¥ï¸ {server} ì‘ë‹µ: {result}\n")
```

<br>

#### 2. ì—¬ëŸ¬ê°œì˜ ì„œë²„PC(ì˜ˆ: 3ëŒ€ GPU ë¨¸ì‹ )ë¥¼ í™œìš©í•´ ìë™ ë¶„ì‚°ì²˜ë¦¬.

```python
import requests, random

OLLAMA_SERVERS = [
  "http://192.168.0.11:11434",
  "http://192.168.0.12:11434",
  "http://192.168.0.13:11434",
]

def ask_ollama(prompt: str):
    server = random.choice(OLLAMA_SERVERS)
    res = requests.post(f"{server}/api/generate", json={
        "model": "mistral:7b",
        "prompt": prompt,
        "options": {"temperature": 0.3}
    })
    return res.json()["response"]

print(ask_ollama("í•œêµ­ AI ì‹œì¥ì˜ íˆ¬ì íŠ¸ë Œë“œë¥¼ ìš”ì•½í•´ì¤˜."))
```


---

## ğŸ’» 3. AI í™œìš©ë°©ì‹ ë¹„êµ

#### ğŸ”¹ OpenAI ë°©ì‹
```python
# =========================================
# âœ… OpenAI GPT-4o-mini ì‹¤ì „ ì˜ˆì‹œ
# =========================================

from openai import OpenAI
import os
from dotenv import load_dotenv

# 1ï¸âƒ£ .env íŒŒì¼ì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸° (.env ë‚´ìš©: OPENAI_API_KEY="sk-xxxxx")
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# 2ï¸âƒ£ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = OpenAI(api_key=api_key)

# 3ï¸âƒ£ ëŒ€í™” ìš”ì²­
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "You are a helpful AI assistant that writes in Korean."},
        {"role": "user", "content": "í•œêµ­ì˜ AI ì‚°ì—… ë™í–¥ì„ í•µì‹¬ë§Œ ìš”ì•½í•´ì¤˜."}
    ],
    temperature=0.3,
    top_p=0.9
)

# 4ï¸âƒ£ ê²°ê³¼ ì¶œë ¥
print(response.choices[0].message.content)
```

1. OpenAI ì„œë²„ì—ì„œ GPU ì—°ì‚°ì´ ì´ë¯¸ ìˆ˜í–‰ë˜ë¯€ë¡œ,
ì‚¬ìš©ì ë¡œì»¬ì— GPUê°€ ì—†ì–´ë„ ê°€ëŠ¥.
2. ë‹¨, í† í° ë‹¨ê°€ê°€ ìˆìœ¼ë¯€ë¡œ ê° API ìš”ì²­(ì§ˆë¬¸/ë‹µë³€) ë§ˆë‹¤ ìš”ê¸ˆì´ ëˆ„ì ë˜ëŠ” ë°©ì‹


#### ğŸ”¹ Ollama ë°©ì‹
```python
import requests

payload = {
  "model": "exaone3.5",
  "messages": [{"role": "user", "content": "í•œêµ­ì˜ AI ì‚°ì—… ë™í–¥ ìš”ì•½"}],
  "options": {"temperature": 0.3, "top_p": 0.9}
}

res = requests.post("http://localhost:11434/api/chat", json=payload)
print(res.json()["message"]["content"])
```
1. ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ LLMì„ ì§ì ‘ PCì— ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì‚¬ìš©. (ì™¸ë¶€ì„œë²„ ì—°ê²° ì—†ì´ êµ¬ë™ê°€ëŠ¥)
2. LLMì´ PCì—ì„œ ì§ì ‘ êµ¬ë™ë˜ê¸° ë•Œë¬¸ì— ê³ ì„±ëŠ¥ GPU í•„ìš” (CUDA toolkit)
<br><br>

| êµ¬ë¶„ | OpenAI (Cloud ê¸°ë°˜) | Ollama (Opensource ê¸°ë°˜)|
|------|---------|---------|
| **API Key í•„ìš”** | âœ… | âŒ |
| **ì‹¤í–‰ ìœ„ì¹˜** | í´ë¼ìš°ë“œ | ë¡œì»¬ |
| **ì‘ë‹µ ì†ë„** | ë¹ ë¦„ (ì„œë²„ ìµœì í™”) | GPU ì˜ì¡´ |
| **ì„±ëŠ¥** | ì„œë²„ ìµœì í™”, ìµœì‹  ëª¨ë¸ | í™˜ê²½ ì˜ì¡´ì  |
| **ë¹„ìš©** | í† í° ê³¼ê¸ˆ | ë¬´ë£Œ (GPU ë¹„ìš©ë§Œ) |
| **ìœ ì§€ë³´ìˆ˜** | ìµœì†Œ | ëª¨ë¸ ê´€ë¦¬ í•„ìš” |
| **ë³´ì•ˆì„±** | ì™¸ë¶€ ì „ì†¡ í•„ìš” | ë‚´ë¶€ ì „ìš© ê°€ëŠ¥(On-Premise) |
| **í™•ì¥ì„±** | ë¬´í•œ | ì„œë²„ ì¦ì„¤ í•„ìš” |
| **ë°ì´í„° ì œì–´** | ë¶ˆê°€ | ì™„ì „ ììœ  |
| **ì í•© ì‚¬ë¡€** | SaaS, ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ | ì‚¬ë‚´ ì±—ë´‡, íì‡„ë§ |

---

## ğŸ§© 4. í†µí•©í˜• RAG ì˜ˆì‹œ (Hybrid êµ¬ì„±)

> OpenAI + Ollama ë¥¼ í˜¼ìš©í•˜ì—¬, í’ˆì§ˆê³¼ ë¹„ìš©ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ë°©ë²•ë²•

```python
"""
Hybrid RAG (OpenAI + Ollama)
- OpenAI: ìµœì¢… ì‘ë‹µ ìƒì„±(ê³ í’ˆì§ˆ)
- Ollama: ì§ˆì˜ í™•ì¥(Multi-Query), ë¬¸ì„œ ì••ì¶•/ì¶”ì¶œ(LLM ê¸°ë°˜ ë¦¬ëŸ¬í‚¹ ëŒ€ìš©)
- Dense(FAISS) + Sparse(BM25) ì•™ìƒë¸” â†’ LLM ì••ì¶• â†’ RetrievalQA

ìš”êµ¬ì‚¬í•­:
- pip install -U "langchain>=0.2" langchain-community langchain-openai langchain-huggingface faiss-cpu
- pip install -U sentence-transformers
- (ì„ íƒ) pip install -U pydantic-settings python-dotenv
- Ollama ì„œë²„ ë™ì‘ í•„ìš”(ì˜ˆ: ollama run exaone3.5 or llama3.2:latest)
- í™˜ê²½ë³€ìˆ˜: OPENAI_API_KEY

í…ŒìŠ¤íŠ¸ ë¬¸ì„œ í´ë”: ./docs
- .txt, .md, .pdf(í…ìŠ¤íŠ¸ë§Œ), .csv ë“± ê°„ë‹¨ ì˜ˆì‹œ ë¬¸ì„œ ë°°ì¹˜
"""
from __future__ import annotations
import os
import sys
from typing import List

# LangChain Core
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.retrievers import EnsembleRetriever
from langchain.schema.runnable import RunnablePassthrough

# Loaders
from langchain_community.document_loaders import DirectoryLoader, TextLoader, CSVLoader, PyPDFLoader

# Vectorstores & Retrievers
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.retrievers import BM25Retriever

# LLMs
from langchain_community.llms import Ollama

# Compression / Reranking (LLM-based)
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.retrievers import ContextualCompressionRetriever

# Multi-Query Retriever
from langchain.retrievers.multi_query import MultiQueryRetriever

# Chains
from langchain.chains import RetrievalQA
from langchain.prompts import ChatPromptTemplate


# -----------------------------
# 0. í™˜ê²½ ì²´í¬
# -----------------------------
def check_env():
    if not os.getenv("OPENAI_API_KEY"):
        print("[WARN] OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. export/setx ë¡œ ì„¤ì •í•˜ì„¸ìš”.")


# -----------------------------
# 1. ë¬¸ì„œ ë¡œë“œ & ì²­í¬ ë¶„í• 
# -----------------------------
def load_documents(doc_dir: str = "./docs") -> List[Document]:
    loaders = []
    if not os.path.isdir(doc_dir):
        os.makedirs(doc_dir, exist_ok=True)
        print(f"[INFO] ë¬¸ì„œ í´ë”ê°€ ì—†ì–´ ìƒì„±í–ˆìŠµë‹ˆë‹¤: {doc_dir}")

    # DirectoryLoader: í…ìŠ¤íŠ¸/ë§ˆí¬ë‹¤ìš´ ìš°ì„ 
    loaders.append(DirectoryLoader(doc_dir, glob="**/*.txt", loader_cls=TextLoader, show_progress=True))
    loaders.append(DirectoryLoader(doc_dir, glob="**/*.md", loader_cls=TextLoader, show_progress=True))

    # PDF (ê°„ë‹¨ í…ìŠ¤íŠ¸ ì¶”ì¶œ)
    try:
        loaders.append(DirectoryLoader(doc_dir, glob="**/*.pdf", loader_cls=PyPDFLoader, show_progress=True))
    except Exception:
        print("[WARN] PyPDFLoader ì‚¬ìš© ë¶ˆê°€. pdf í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤.")

    # CSV (ì˜µì…˜)
    try:
        loaders.append(DirectoryLoader(doc_dir, glob="**/*.csv", loader_cls=CSVLoader, show_progress=True))
    except Exception:
        pass

    docs: List[Document] = []
    for l in loaders:
        try:
            docs.extend(l.load())
        except Exception as e:
            print(f"[WARN] ì¼ë¶€ ë¡œë” ì‹¤íŒ¨: {type(l).__name__}: {e}")

    if not docs:
        print("[WARN] ./docs ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì˜ˆì‹œ íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")

    # ìŠ¤í”Œë¦¬í„° (ë¬¸ë§¥ ë³´ì¡´)
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,
        chunk_overlap=120,
        separators=["\n## ", "\n# ", "\n\n", "\n", " "]
    )
    chunks = splitter.split_documents(docs)
    print(f"[INFO] ë¡œë“œëœ ë¬¸ì„œ: {len(docs)}ê°œ â†’ ì²­í¬: {len(chunks)}ê°œ")
    return chunks


# -----------------------------
# 2. ì„ë² ë”© & ë²¡í„°ìŠ¤í† ì–´ (Dense)
# -----------------------------
def build_dense_retriever(chunks: List[Document], use_openai_embeddings: bool = True):
    if use_openai_embeddings:
        embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
    else:
        # ë¡œì»¬ ë¬´ë£Œ ì„ë² ë”©(e5) â€” ì†ë„ëŠ” ëŠë¦¬ì§€ë§Œ ë¹„ìš© 0
        embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-base")

    vs = FAISS.from_documents(chunks, embeddings)
    dense_retriever = vs.as_retriever(search_type="mmr", search_kwargs={"k": 8, "fetch_k": 24, "lambda_mult": 0.3})
    return dense_retriever


# -----------------------------
# 3. Sparse(BM25) + Dense ì•™ìƒë¸”
# -----------------------------
def build_ensemble_retriever(chunks: List[Document]):
    # BM25 (í‚¤ì›Œë“œ ê¸°ë°˜)
    bm25 = BM25Retriever.from_documents(chunks)
    bm25.k = 8

    # Dense (OpenAI or HF)
    dense = build_dense_retriever(chunks, use_openai_embeddings=True)

    # ì•™ìƒë¸”: RRF/ê°€ì¤‘ í•©ì„± ìœ ì‚¬ â€” LangChain ì œê³µ EnsembleRetriever
    ensemble = EnsembleRetriever(retrievers=[bm25, dense], weights=[0.45, 0.55])
    return ensemble


# -----------------------------
# 4. Multi-Query with Ollama (ì§ˆì˜ í™•ì¥)
# -----------------------------
def build_multiquery_retriever(base_retriever, ollama_model: str = "exaone3.5"):
    mq_llm = Ollama(model=ollama_model, temperature=0.2)
    mqr = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=mq_llm,
                                       prompt=ChatPromptTemplate.from_template(
                                           """
                                           ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë” ì˜ ê²€ìƒ‰í•˜ê¸° ìœ„í•´, ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì˜ í•œêµ­ì–´ í•˜ìœ„ ì§ˆì˜ 4ê°œë¥¼ ê°„ê²°íˆ ìƒì„±í•˜ë¼.
                                           - ì˜ë¯¸ì ìœ¼ë¡œ ë‹¤ì–‘í•œ í‘œí˜„ ì‚¬ìš©
                                           - ë™ì˜ì–´Â·ê´€ë ¨ê°œë… í¬í•¨
                                           - ê° ì§ˆì˜ëŠ” í•œ ì¤„
                                           ì§ˆë¬¸: {question}
                                           """
                                       ))
    return mqr


# -----------------------------
# 5. LLM ê¸°ë°˜ ì••ì¶• ë¦¬íŠ¸ë¦¬ë²„ (Ollama)
# -----------------------------
def build_compression_retriever(base_retriever, ollama_model: str = "exaone3.5"):
    ollama_llm = Ollama(model=ollama_model, temperature=0.0)
    compressor = LLMChainExtractor.from_llm(ollama_llm)
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=base_retriever,
    )
    return compression_retriever


# -----------------------------
# 6. ìµœì¢… QA ì²´ì¸ (OpenAI ìƒì„±)
# -----------------------------
def build_qa_chain(retriever):
    openai_llm = ChatOpenAI(model="gpt-5-mini", temperature=0.2)

    system_prompt = (
        "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ì„œë“¤ë§Œ í™œìš©í•˜ì—¬ ì •í™•í•˜ê³  ê°„ê²°í•œ í•œêµ­ì–´ ë‹µë³€ì„ ì‘ì„±í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
        "ì¶”ì¸¡ì€ ê¸ˆì§€í•˜ê³ , ì¶œì²˜ ê·¼ê±°ë¥¼ ìš”ì•½í•´ì„œ ë‹µë³€ ëì— bulletë¡œ ë‚˜ì—´í•˜ì„¸ìš”."
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "ì§ˆë¬¸: {question}\n\n" \
                   "ë‹¤ìŒì€ ê²€ìƒ‰ìœ¼ë¡œ íšŒìˆ˜ëœ ë¬¸ì„œì˜ í•µì‹¬ ë°œì·Œì…ë‹ˆë‹¤:\n{context}\n\n" \
                   "ìœ„ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. ë¶ˆí™•ì‹¤í•˜ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”."),
    ])

    chain = RetrievalQA.from_chain_type(
        llm=openai_llm,
        retriever=retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True,
    )
    return chain


# -----------------------------
# 7. í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ êµ¬ì„± í—¬í¼
# -----------------------------
def build_hybrid_pipeline(docs_dir: str = "./docs",
                          ollama_model: str = "exaone3.5"):
    chunks = load_documents(docs_dir)

    # 1) Dense+BM25 ì•™ìƒë¸”
    ensemble = build_ensemble_retriever(chunks)

    # 2) Multi-Queryë¡œ ì§ˆì˜ ë‹¤ì–‘í™” (Ollama)
    mqr = build_multiquery_retriever(ensemble, ollama_model)

    # 3) LLM ì••ì¶• ë¦¬íŠ¸ë¦¬ë²„ (Ollama)
    compressed = build_compression_retriever(mqr, ollama_model)

    # 4) ìµœì¢… QA ì²´ì¸ (OpenAI)
    qa_chain = build_qa_chain(compressed)
    return qa_chain


# -----------------------------
# 8. ì‹¤í–‰ ì˜ˆì‹œ
# -----------------------------
def demo():
    check_env()
    qa = build_hybrid_pipeline(docs_dir="./docs", ollama_model="exaone3.5")

    queries = [
        "ìŠ¤ë§ˆíŠ¸ê·¸ë¦°ì‚°ë‹¨ RAG íŒŒì´í”„ë¼ì¸ì˜ ì¥ì ê³¼ ë‹¨ì ì€?",
        "ë¬¼ë¥˜ ë°ì´í„°ì—ì„œ ì„¸ì…˜ ê¸°ë°˜ ë³¼ë¥¨ í´ëŸ¬ìŠ¤í„°ê°€ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ”?",
        "ì´ í”„ë¡œì íŠ¸ì˜ ë°°í¬ êµ¬ì¡°(í´ë¼ìš°ë“œ/ì˜¨í”„ë ˜)ë¥¼ ìš”ì•½í•´ì¤˜",
    ]

    for q in queries:
        print("\n==============================")
        print("[QUERY]", q)
        result = qa({"query": q})
        print("[ANSWER]\n", result["result"])  # ìµœì¢… ë‹µë³€
        print("[SOURCES]")
        for i, d in enumerate(result.get("source_documents", [])[:5], 1):
            m = d.metadata or {}
            print(f"  {i}. {m.get('source', 'unknown')}  (p:{m.get('page', '-')})")


# -----------------------------
# 9. ê³ ê¸‰: ë¦¬ìŠ¤í¬/ì„±ëŠ¥ íŒ
# -----------------------------
"""
- ë¹„ìš©ìµœì í™”: ì„ë² ë”©ì€ HF(e5)ë¡œ êµ¬ì¶•, ì‘ë‹µë§Œ OpenAI. ë°˜ëŒ€ë¡œ í’ˆì§ˆ í•„ìš”ì‹œ ì„ë² ë”©ë„ OpenAI.
- ì†ë„ìµœì í™”: search_kwargsì˜ k/fetch_k ì¡°ì •, chunk_size ì¤„ì´ê¸°, BM25 ê°€ì¤‘ì¹˜ ê°•í™”.
- í’ˆì§ˆ: Multi-Query ê°œìˆ˜ 3â†’6, Compressor í…œí”Œë¦¿ì„ Q/A íŠ¹í™”ë¡œ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ.
- ìºì‹±: FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œì»¬ì— ì €ì¥/ë¡œë“œ(vs.save_local/load_local).
- ìŠ¤íŠ¸ë¦¬ë°: ChatOpenAI(streaming=True) + ì½œë°± í•¸ë“¤ëŸ¬ ì‚¬ìš©.
- ì—ëŸ¬í•¸ë“¤ë§: Ollama ì„œë²„ ë¯¸ë™ì‘ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬(ëŒ€ì²´ ê²½ë¡œ: MQR ìƒëµ + Ensembleë§Œ).
"""


if __name__ == "__main__":
    try:
        demo()
    except KeyboardInterrupt:
        print("\n[INFO] Interrupted by user.")
    except Exception as e:
        print(f"[ERROR] {e}")
        sys.exit(1)
```

---

### ğŸ’» (2) API í™œìš© ë°©ì‹ ë¹„êµ

